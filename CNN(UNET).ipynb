{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN(UNET).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_dBGjk6ZhJZ",
        "colab_type": "text"
      },
      "source": [
        "#Convolutional Neural Network(UNET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI9RbdiIK3yO",
        "colab_type": "text"
      },
      "source": [
        "**Importing Libraries**\n",
        "\n",
        "Load the dependencies and modules required for the rest of the script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n51kR89o0yGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plot\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkkpRR-MpPK5",
        "colab_type": "text"
      },
      "source": [
        "**Initialization of Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kcsg2Hc1Ag-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "width=160\n",
        "height=120\n",
        "i=0\n",
        "taille_batch=50\n",
        "nbr_entrainement=100\n",
        "learning_rate=1E-3\n",
        "nbr_mask=13\n",
        "momentum=0.99 \n",
        "tab_img=[]\n",
        "tab_mask=[]\n",
        "img_mask=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1qniYkpd_h",
        "colab_type": "text"
      },
      "source": [
        "**Loading images and masks**\n",
        "\n",
        " https://www.kaggle.com/kumaresanmanickavelu/lyft-udacity-challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxKvdk-V1GeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_img=\"CameraRGB/\"\n",
        "dir_mask=\"CameraSeg/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdhFrLb11Vfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dir in ['/content/drive/My Drive/Colab Notebooks/dataset/lyft-udacity-challenge/resized/dataA/']:\n",
        "  for file in os.listdir(dir+dir_img):\n",
        "    i=i+1\n",
        "    print(i)\n",
        "    tab_img.append(cv2.imread(dir+dir_img+file)/255)\n",
        "    img_mask=cv2.imread(dir+dir_mask+file)[:,:,2]\n",
        "    img_mask_result=np.zeros(shape=( height,width,13), dtype=np.float32)\n",
        "    img_mask_result[:,:,0][img_mask==1]=1.\n",
        "    img_mask_result[:,:,1][img_mask==2]=1.\n",
        "    img_mask_result[:,:,2][img_mask==3]=1.\n",
        "    img_mask_result[:,:,3][img_mask==4]=1.\n",
        "    img_mask_result[:,:,4][img_mask==5]=1.\n",
        "    img_mask_result[:,:,5][img_mask==6]=1.\n",
        "    img_mask_result[:,:,6][img_mask==7]=1.\n",
        "    img_mask_result[:,:,7][img_mask==8]=1.\n",
        "    img_mask_result[:,:,8][img_mask==9]=1.\n",
        "    img_mask_result[:,:,9][img_mask==10]=1.\n",
        "    img_mask_result[:,:,10][img_mask==11]=1.\n",
        "    img_mask_result[:,:,11][img_mask==12]=1.\n",
        "    img_mask_result[:,:,12][img_mask==13]=1.\n",
        "    tab_mask.append(img_mask_result)\n",
        "    if i==500:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_uq9U0IqjYt",
        "colab_type": "text"
      },
      "source": [
        "**Create the Placeholders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puc-mdfSGyYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ph_images=tf.placeholder(shape=(None, 120,160, 3), dtype=tf.float32,name='images') #3 Colors\n",
        "ph_masks=tf.placeholder(shape=(None,120,160,13), dtype=tf.float32)  #13 Classes\n",
        "ph_is_training=tf.placeholder_with_default(False, (), name='is_training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4XVRiSqFU-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tab_img=np.array(tab_img)\n",
        "tab_mask=np.array(tab_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URP2huPxFXTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images, test_images, train_labels, test_labels=train_test_split(tab_img, tab_mask, test_size=.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMV4Djk7qwsV",
        "colab_type": "text"
      },
      "source": [
        "**You can create your own Normalization function or use \"tf.layers.batch_normalization\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffTKo2C4Fbc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def normalisation(couche_prec):\n",
        "    #mean, var=tf.nn.moments(couche_prec, [0])\n",
        "    #scale=tf.Variable(tf.ones(shape=(np.shape(couche_prec)[-1])))\n",
        "    #beta=tf.Variable(tf.zeros(shape=(np.shape(couche_prec)[-1])))\n",
        "    #result=tf.nn.batch_normalization(couche_prec, mean, var, beta, scale, 0.001)\n",
        "    #return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjx3LhoYrMWG",
        "colab_type": "text"
      },
      "source": [
        "**Convolution fonction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOHH-d8FGBk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_conv(couche_prec,taille_noyau, nbr_noyau,stride,training=False):\n",
        "    # First convolution\n",
        "    conv_W = tf.Variable(tf.truncated_normal(shape=(taille_noyau, taille_noyau, int(couche_prec.get_shape()[-1]), nbr_noyau))) \n",
        "    conv_b = tf.Variable(tf.zeros(nbr_noyau))\n",
        "    conv   = tf.nn.conv2d(couche_prec, conv_W, strides=[1,stride,stride, 1], padding='SAME') + conv_b\n",
        "    conv=tf.layers.batch_normalization(conv, training=training,momentum=momentum)\n",
        "    return conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZv0J4LPraXT",
        "colab_type": "text"
      },
      "source": [
        "**Deconvolution fonction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVAt3X17GioX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deconvolution(input, taille_noyau, nbr_conv, stride,training=False):\n",
        "    w=tf.Variable(tf.random.truncated_normal(shape=(taille_noyau, taille_noyau, nbr_conv, int(input.get_shape()[-1])))) \n",
        "    b=np.zeros(nbr_conv)\n",
        "    out_h=int(input.get_shape()[1])*stride\n",
        "    out_w=int(input.get_shape()[2])*stride\n",
        "    b_size=tf.shape(input)[0]\n",
        "    result=tf.nn.conv2d_transpose(input, w, output_shape=[b_size, out_h, out_w, nbr_conv], strides=[1, stride, stride, 1], padding='SAME') #declaration de la g√©ometrie de la couche de sortie\n",
        "    result=result+ b\n",
        "    result=tf.layers.batch_normalization(result, training=training,momentum=momentum)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwWM71o8rmoi",
        "colab_type": "text"
      },
      "source": [
        "**Building UNET Architecture**\n",
        "\n",
        "The comments indicate the size of the images after each convolution to help you understand well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvIBz_EkJht3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C1 = create_conv(ph_images,3,16,1,ph_is_training) #C1 (160*120*16)\n",
        "C1 = tf.nn.relu(C1)   # Activation: relu\n",
        "\n",
        "C2 = create_conv(C1,3, 16,1,ph_is_training) #C2 (160*120*16)\n",
        "C2 = tf.nn.relu(C2)\n",
        "\n",
        "C3 = tf.nn.max_pool(C2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # Pooling1\n",
        "\n",
        "C3 = create_conv(C3,3, 32,1,ph_is_training) #C3 (80*60*32)\n",
        "C3= tf.nn.relu(C3)\n",
        "\n",
        "C4 = create_conv(C3,3, 32,1,ph_is_training) #C4 (80*60*32)\n",
        "C4 = tf.nn.relu(C4)\n",
        "\n",
        "C5= tf.nn.max_pool(C4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # Pooling2\n",
        "\n",
        "C5 = create_conv(C5,3, 64,1,ph_is_training) #C5 (40*30*64)\n",
        "C5 = tf.nn.relu(C5)\n",
        "\n",
        "C6 = create_conv(C5,3, 64,1,ph_is_training) #C6 (40*30*64)\n",
        "C6 = tf.nn.relu(C6)\n",
        "\n",
        "C7 = tf.nn.max_pool(C6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # Pooling3\n",
        "\n",
        "C7 = create_conv(C7,3, 128,1,ph_is_training) #C7 (20*15*128)\n",
        "C7 = tf.nn.relu(C7)\n",
        "\n",
        "C8 = create_conv(C7,3, 128,1,ph_is_training) #C8 (20*15*128)\n",
        "C8= tf.nn.relu(C8)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f9hVDa3WChG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D1=deconvolution(C8,3,64,2,ph_is_training) #D1 \n",
        "D1= tf.nn.relu(D1)\n",
        "result=tf.concat((D1, C6), axis=3)\n",
        "result=create_conv(result,3,64,1,ph_is_training) #C1 (40*30*64)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "result=create_conv(result,3,64,1,ph_is_training) #C2 (40*30*64)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "D2=deconvolution(result,3,64,2,ph_is_training) #D2\n",
        "result= tf.nn.relu(D2)\n",
        "result=tf.concat((D2, C4), axis=3)\n",
        "result=create_conv(result,3,32,1,ph_is_training) #C3 (80*60*32)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "result=create_conv(result,3,32,1,ph_is_training) #C4 (80*60*32)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "D3=deconvolution(result,3,32,2,ph_is_training) #D3\n",
        "result= tf.nn.relu(D3)\n",
        "result=tf.concat((D3, C2), axis=3)\n",
        "result=create_conv(result,3,16,1,ph_is_training) #C5 (160*120*16)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "result=create_conv(result,3,16,1,ph_is_training) #C6 (160*120*16)\n",
        "result = tf.nn.relu(result)\n",
        "\n",
        "w=tf.Variable(tf.random.truncated_normal(shape=(1,1,int(result.get_shape()[-1]),nbr_mask)))\n",
        "b=np.zeros(nbr_mask)\n",
        "result= tf.nn.conv2d(result,w,strides=[1,1,1,1], padding='SAME')\n",
        "result=result +b \n",
        "mask=tf.nn.sigmoid(result,name=\"sortie\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAQvwY0Cr2sJ",
        "colab_type": "text"
      },
      "source": [
        "**Model training and optimization**\n",
        "\n",
        "-For the loss function, we must use cross entropy with **Sigmoid** instead of **Softmax**, because the input is not a vector of probabilities but a single value of probability which equals 1 if the pattern is present and 0 otherwise.\n",
        "\n",
        "-Likewise for the precision calculation, instead of **argmax** we use **round** which returns 1 if the probability is> 0.5 and 0 otherwise.\n",
        "\n",
        "-The optimization method **AdamOptimizer** performs better than **GradientDescent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycqM7m1IhzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=ph_masks, logits=result) \n",
        "train=tf.train.AdamOptimizer(learning_rate).minimize(loss) \n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.round(mask), ph_masks), tf.float32)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjeTW_AjZweC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver=tf.train.Saver()\n",
        "with tf.Session() as s:\n",
        "  s.run(tf.global_variables_initializer())\n",
        "  tab_train=[]\n",
        "  tab_test=[]\n",
        "  for id_entrainement in np.arange(nbr_entrainement):\n",
        "      print(\"> Entrainement\", id_entrainement)\n",
        "      for batch in np.arange(0, len(train_images), taille_batch):\n",
        "          s.run(train, feed_dict={\n",
        "              ph_images: train_images[batch:batch+taille_batch],\n",
        "              ph_masks: train_labels[batch:batch+taille_batch],\n",
        "              ph_is_training: True\n",
        "          })\n",
        "      print(\"  entrainement OK\")\n",
        "      tab_accuracy_train=[]\n",
        "      for batch in np.arange(0, len(train_images), taille_batch):\n",
        "          p=s.run(accuracy, feed_dict={\n",
        "              ph_images: train_images[batch:batch+taille_batch],\n",
        "              ph_masks: train_labels[batch:batch+taille_batch]\n",
        "          })\n",
        "          tab_accuracy_train.append(p)\n",
        "      print(\"  train:\", np.mean(tab_accuracy_train))\n",
        "      tab_accuracy_test=[]\n",
        "      for batch in np.arange(0, len(test_images), taille_batch):\n",
        "          p=s.run(accuracy, feed_dict={\n",
        "              ph_images: test_images[batch:batch+taille_batch],\n",
        "              ph_masks: test_labels[batch:batch+taille_batch]\n",
        "          })\n",
        "          tab_accuracy_test.append(p)\n",
        "      print(\"  test :\", np.mean(tab_accuracy_test))\n",
        "      tab_train.append(1-np.mean(tab_accuracy_train))\n",
        "      tab_test.append(1-np.mean(tab_accuracy_test))\n",
        "  #Affichage des deux courbes de performance \n",
        "  plot.ylim(0, 1)\n",
        "  plot.grid()\n",
        "  plot.plot(tab_train, label=\"Train error\")\n",
        "  plot.plot(tab_test, label=\"Test error\")\n",
        "  plot.legend(loc=\"upper right\")\n",
        "  plot.show()\n",
        "  resulat=s.run(mask, feed_dict={ph_images: test_images[0:taille_batch]})\n",
        "  np.set_printoptions(formatter={'float': '{:0.3f}'.format})\n",
        "  for image in range(taille_batch):\n",
        "    print(\"image\", image)\n",
        "    print(\"sortie du r√©seau:\", resulat[image], np.argmax(resulat[image]))\n",
        "    print(\"sortie attendue :\", test_labels[image], np.argmax(test_labels[image]))\n",
        "    cv2_imshow(test_images[image]*255)\n",
        "  #Sauvegarde du mod√®le\n",
        "  saver.save(s, './drive/My Drive/modele')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}